{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segment Road and Sidewalk\n",
    "Tony Wang July 04 2023\n",
    "\n",
    "After semantic segmentation of road and sidewalk, we obtain the pixel level binary mask of them. Which can be used to detect human-road relationship using rule-based comparision. Since the SAM didn't provide necessary api, I write some utility func to realize it\n",
    "\n",
    "> This notebook is used for tutuorial demo, because I believe, compared to the unstable .py file, jupyter notebook would provide a vivid description and data pipeline demonstration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "# filter some annoying debug info\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import supervision as sv\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "#TODO name!\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "# import SAM_utility # \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Paths to GroundingDINO and SAM checkpoints\n",
    "GROUNDING_DINO_CONFIG_PATH = \"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = \"./weights/groundingdino_swint_ogc.pth\"\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "SAM_CHECKPOINT_PATH = \"./weights/sam_vit_b_01ec64.pth\"\n",
    "\n",
    "# Predict classes and hyper-param for GroundingDINO\n",
    "BOX_TRESHOLD = 0.25\n",
    "TEXT_TRESHOLD = 0.25\n",
    "PED_TRESHOLD = 0.5\n",
    "\n",
    "NMS_THRESHOLD = 0.85\n",
    "IOU_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "# DEBUG = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from depth_util import predict_depth,get_distance_category\n",
    "from mask_util import (\n",
    "    show_mask, show_points, show_box, display_mask, nms_processing, \n",
    "    is_overlap, compute_overlap, get_location, get_surface_info\n",
    ")\n",
    "from file_io    import is_image_file\n",
    "from angle_util import describe_angle,estimate_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DPT_module.dpt.models import DPTDepthModel\n",
    "from DPT_module.dpt.midas_net import MidasNet_large\n",
    "from DPT_module.dpt.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "import DPT_module.util.io as DPT_io\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model loading is quite long\n",
    "with some unremovable warning in gDINO, just ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Initialize GroundingDINO model\n",
    "grounding_dino_model = Model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Initialize SAM model and predictor\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "LocationInfo: pack form to help data-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationInfo:\n",
    "    def __init__(self, object_type, id, box, mask,confidence):\n",
    "        self.object_type = object_type  # ('sidewalk', 'road', or 'person')\n",
    "        self.id = id  # Unique ID within the type\n",
    "        self.box = box  # Bounding box in xyxy format\n",
    "        self.mask = mask  # Binary mask indicating the precise location of the object\n",
    "        self.confidence = confidence #confidence of bbox\n",
    "        self.distance = None # str,{very close,close, median, far, very far}\n",
    "        self.dist_value = None\n",
    "        self.angle = None    # horizontal angle relative to camera\n",
    "    def get_area(self):\n",
    "        \"\"\"\n",
    "        int: The area of the object in pixels.\n",
    "        \"\"\"\n",
    "        return np.sum(self.mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Architecture:\n",
    "1. gDINO : grounding_dino_model.predict_with_classes\n",
    "\n",
    "   CLASSES_prompt= ['road', 'sidewalk']\n",
    "\n",
    "   Based on testing, this pair is most reliable (otherwise the sidewalk may messed up with road) \n",
    "\n",
    "   In this part, I use the box as Region of Interest(ROI) to further prompt SAM\n",
    "\n",
    "2. Non-maximum suppression (NMS) :\n",
    "\n",
    "   get rid of redundant and overlapping bounding boxes.\n",
    "\n",
    "   the metric is Intersection over Union(IoU)\n",
    "\n",
    "3. Prompting SAM with ROI, select mask with largest area, in this step, the road and sidewalk can be segmented with naming in pixel level accuracy.\n",
    "\n",
    "4. save the result \n",
    "\n",
    "5. label the result with label and confidence\n",
    "\n",
    "6. TODO: do image sequence experiment, analyze the behavior of person\n",
    "\n",
    "7. TODO: split cases based on JAAD info\n",
    "\n",
    "   - car is moving \n",
    "   - car is stopping\n",
    "   - time\n",
    "   - weather\n",
    "   - more...\n",
    "\n",
    "In GTX3090 environment, the algorithm runs relatively fast with GPU boosting.\n",
    "\n",
    "(Not as bad as I guessed before, much faster than all of the online demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location utility function\n",
    "- is_overlap: a mask-level comparitor func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_print(obj_dict,image):\n",
    "    for name, person in obj_dict.items():\n",
    "        print(name)\n",
    "        print(person.box)\n",
    "        print(person.distance)\n",
    "        person.angle = estimate_angle(image,person)\n",
    "\n",
    "        print(f\"angle is {person.angle},it is {describe_angle(person.angle)}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_txt(image_path, output_path, p_surface_overlaps,obj_dict):\n",
    "    \"\"\"\n",
    "    Writes the details of a road scene analysis into a text file. \n",
    "    The information includes:\n",
    "    detected persons and the surfaces they are on, counts of various detections,\n",
    "    labels for detections and people, and details about each object in the scene.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        image_path (str): The relative path to the input image.\n",
    "        output_path (str): The output path for the analysis results.\n",
    "        p_surface_overlaps (List[Tuple[LocationInfo, List[LocationInfo]]])\n",
    "        : A list of tuples where each tuple contains a person\n",
    "        and a list of surfaces (as LocationInfo objects) that the person overlaps with.\n",
    "        counts (Tuple[int, int, int, int]): A tuple containing counts of surface masks,\n",
    "        road & sidewalk masks, people's masks, and actual people.\n",
    "        labels (List[str]): A list of labels for the detected objects.\n",
    "        p_labels (List[str]): A list of labels specifically for the detected persons.\n",
    "        obj_dict (Dict[str, LocationInfo]) A dictionary mapping from unique entity identifiers \n",
    "        to corresponding LocationInfo objects.\n",
    "\n",
    "    Returns & Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_path).parent\n",
    "    img_name = image_path[-8:-4]\n",
    "    txt_name = \"Info_Video_\"+ str(output_dir)[-4:] +\".txt\"\n",
    "    txt_path = os.path.join(output_dir, txt_name) \n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"output_dir: \", output_dir)\n",
    "        print(\"image_name: \", img_name)\n",
    "        print(\"txt_path: \", txt_path)\n",
    "    # Check if file already exists\n",
    "    if DEBUG:\n",
    "        if os.path.exists(txt_path):\n",
    "            # Read in existing data\n",
    "            with open(txt_path, 'r') as f:\n",
    "                existing_data = f.read()\n",
    "\n",
    "            # If the info of the current image has already been recorded, return without appending\n",
    "            if f\"INFO of frame {img_name}:\\n\" in existing_data:\n",
    "                print(f\"ERROR: the info of{img_name} has been generated\")\n",
    "                return\n",
    "    with open(txt_path, 'a') as f: # 'a' option is for appending to the file if it exists\n",
    "        f.write(f\"INFO of frame {img_name}:\\n\")\n",
    "\n",
    "        get_surface_info(obj_dict,f)\n",
    "        \n",
    "        for person, surfaces in p_surface_overlaps:\n",
    "            if surfaces:\n",
    "                surface_str = ', '.join([f\"{surface.object_type} {surface.id}\" for surface in surfaces])\n",
    "                f.write(f\"Person {person.id} is on the {surface_str}, his/her bbox is{person.box}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Person {person.id} is not on any detected surface, his/her bbox is{person.box}\\n\")\n",
    "                \n",
    "        # f.write(f\"Labels: [{', '.join(labels)}]\\n\")\n",
    "        # f.write(f\"Person Labels: [{', '.join(p_labels)}]\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_ROI(sam_predictor: SamPredictor, image: np.ndarray, boxes: np.ndarray):\n",
    "    # Prompting SAM with Region of Interest\n",
    "    # Return: binary mask indicating the precise shape of object\n",
    "\n",
    "    sam_predictor.set_image(image)\n",
    "    result_masks = []\n",
    "    for box in boxes:\n",
    "        masks_np, iou_predictions, _ = sam_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=box,\n",
    "        multimask_output=True,\n",
    "        )\n",
    "        #TODO Remove the following line to get all the person masks\n",
    "        # index = np.argmax(scores_np) \n",
    "        # Add all masks to the result, not just the one with the highest score\n",
    "        # Filter out masks with IoU scores below the threshold\n",
    "        for mask, score in zip(masks_np, iou_predictions):\n",
    "            if score >= IOU_THRESHOLD:\n",
    "                result_masks.append(mask)\n",
    "\n",
    "    return np.array(result_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refactored version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cv2_image(image_path: str):\n",
    "#     \"\"\"\n",
    "#     This function loads an image from a given path and converts it from BGR to RGB.\n",
    "\n",
    "#     Args:\n",
    "#     - image_path: The path to the image.\n",
    "\n",
    "#     Returns:\n",
    "#     - The converted image if successful, or None if the image could not be loaded or processed.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         image = cv2.imread(image_path)\n",
    "#         if image is None:\n",
    "#             print(f\"Image at path {image_path} could not be loaded. Skipping.\")\n",
    "#             return None\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         return image\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to process image at {image_path}. Error: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_detections(image, box_threshold, text_threshold, classes):\n",
    "#     \"\"\"\n",
    "#     This function runs the detection model and performs non-maximum suppression on the results.\n",
    "\n",
    "#     Args:\n",
    "#     - image: The input image.\n",
    "#     - box_threshold: The confidence threshold for considering detection boxes.\n",
    "#     - text_threshold: The confidence threshold for considering detected labels.\n",
    "#     - classes: The classes to detect.\n",
    "\n",
    "#     Returns:\n",
    "#     - detections: The detected objects after non-maximum suppression.\n",
    "#     \"\"\"\n",
    "#     detections = grounding_dino_model.predict_with_classes(\n",
    "#         image=image,\n",
    "#         classes=classes,\n",
    "#         box_threshold=box_threshold,\n",
    "#         text_threshold=text_threshold\n",
    "#     )\n",
    "#     detections = nms_processing(detections)\n",
    "\n",
    "#     return detections\n",
    "\n",
    "# def get_label_and_box(classes,detections):\n",
    "    \n",
    "#     labels = [\n",
    "#         f\"{classes[class_id]} {i} {confidence:0.2f}\" \n",
    "#         for i, (_, _, confidence, class_id, _) in enumerate(detections)]\n",
    "#     boxes = np.array(detections.xyxy)\n",
    "#     return labels,boxes\n",
    "\n",
    "# def get_location_info(boxes, labels, masks):\n",
    "#     \"\"\"\n",
    "#     This function creates LocationInfo objects for the detected objects.\n",
    "\n",
    "#     Args:\n",
    "#     - boxes: The detected objects.\n",
    "#     - labels: The labels for the detected objects.\n",
    "#     - masks: The segmentation masks for the detected objects.\n",
    "#     - object_type: The type of the objects.\n",
    "\n",
    "#     Returns:\n",
    "#     - obj_dict: A dictionary mapping from unique entity identifiers to corresponding LocationInfo objects.\n",
    "#     \"\"\"\n",
    "#     obj_dict = dict()\n",
    "\n",
    "#     for i, (box, label, mask) in enumerate(zip(boxes, labels, masks)):\n",
    "#         object_type, id, confidence   = label.split(' ')\n",
    "#         index = object_type + id\n",
    "#         obj_dict[index] =  (LocationInfo(object_type, int(id), box, mask, confidence)) \n",
    "\n",
    "#     return obj_dict\n",
    "\n",
    "# def analyze_person_location(obj_dict, depth_flag, image=None, depth_map=None):\n",
    "#     \"\"\"\n",
    "#     This function analyzes where each person is standing.\n",
    "\n",
    "#     Args:\n",
    "#     - obj_dict: A dictionary of LocationInfo objects.\n",
    "#     - depth_flag: A flag indicating whether to predict the depth.\n",
    "#     - image: The input image.\n",
    "#     - depth_map: The depth map of the scene.\n",
    "\n",
    "#     Returns:\n",
    "#     - p_surface_overlaps: A list of tuples where each tuple contains a person and the surfaces they overlap with.\n",
    "#     \"\"\"\n",
    "#     p_surface_overlaps = []\n",
    "    \n",
    "#     for name, person in obj_dict.items():\n",
    "#         if person.object_type != \"person\":\n",
    "#             continue # We only want to analyze persons\n",
    "\n",
    "#         if depth_flag:\n",
    "#             person.distance = get_distance_category(depth_map,person.mask)\n",
    "#             person.angle   = estimate_angle(image,person)\n",
    "        \n",
    "#         overlaps = []\n",
    "#         for name, surface in obj_dict.items():\n",
    "#             # We only want to analyze surfaces (road or sidewalk)\n",
    "#             if surface.object_type not in ['road', 'sidewalk']: \n",
    "#                 continue\n",
    "\n",
    "#             # Check if the person and the surface overlap\n",
    "#             overlap, _ = is_overlap(person.mask, surface.mask)\n",
    "#             if overlap:\n",
    "#                 overlaps.append(surface)\n",
    "\n",
    "#         p_surface_overlaps.append((person, overlaps))\n",
    "\n",
    "#     return p_surface_overlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_annotated_image(image, labels,P_labels,detections,p_detections):\n",
    "\n",
    "#     box_annotator = sv.BoxAnnotator()\n",
    "#     person_annotator = sv.BoxAnnotator()\n",
    "\n",
    "#     annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections ,labels=labels)\n",
    "#     if DEBUG:\n",
    "#         sv.plot_image(annotated_frame, (16, 9))\n",
    "#     person_annotation = person_annotator.annotate(scene=annotated_frame,detections= p_detections,labels= P_labels)\n",
    "#     if DEBUG:\n",
    "#         sv.plot_image(person_annotation, (16, 9))\n",
    "\n",
    "#     return person_annotation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_road(image_path:str, output_path:str):\n",
    "#     image = get_cv2_image(image_path)\n",
    "#     if image is None:\n",
    "#         print(\"Error when reading \",image_path)\n",
    "#         return None\n",
    "#     ROAD_SIDEWALK = ['road', 'sidewalk'] \n",
    "#     P_CLASS     = ['person'] #,'bike']\n",
    "#     # Detect objects\n",
    "#     detections = get_detections(image, BOX_TRESHOLD, TEXT_TRESHOLD,ROAD_SIDEWALK)\n",
    "#     p_detections = get_detections(image, BOX_TRESHOLD, PED_TRESHOLD, P_CLASS)\n",
    "#     if detections is not None:\n",
    "#         labels,DINO_boxes = get_label_and_box(ROAD_SIDEWALK,detections)\n",
    "#         print(\"no road detected\")\n",
    "#     if p_detections is not None:\n",
    "#         P_labels, P_boxes = get_label_and_box(P_CLASS,p_detections)\n",
    "#     else:\n",
    "#         print(\"no pedestrian detected\")\n",
    "#     # Segment objects\n",
    "#     SAM_masks = segment_ROI(sam_predictor, image, DINO_boxes)\n",
    "#     P_masks = segment_ROI(sam_predictor, image, P_boxes)\n",
    "    \n",
    "#     person_annotation = get_annotated_image(image, labels,P_labels,detections,p_detections)\n",
    "\n",
    "#     # Create LocationInfo objects\n",
    "#     obj_dict = get_location_info(DINO_boxes, labels, SAM_masks)\n",
    "#     obj_dict.update(get_location_info(P_boxes, P_labels, P_masks))\n",
    "\n",
    "\n",
    "#     counts = display_mask(SAM_masks,P_masks,P_boxes,DINO_boxes,person_annotation,output_path)\n",
    "\n",
    "#     return obj_dict, counts,labels, P_labels\n",
    "\n",
    "\n",
    "# #\n",
    "# inputfile = \"Frame_3hz_ARHUD/video_0311/image_0009.png\"\n",
    "# outputfile = \"../video_0311/image_0009.png\"\n",
    "\n",
    "# # output_dir = Path(outputfile).parent\n",
    "# # output_dir.mkdir(parents=True, exist_ok=True)\n",
    "# obj_dict, counts,labels, P_labels = detect_road(\"input/video_0310/image_0005.png\",output_path=\"../video_0310/image_0005.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process and write results\n",
    "# def analyze_road(image_path,output_path,counts, labels, P_labels,depth_flag=False):\n",
    "#     image = get_cv2_image(image_path)\n",
    "#     if image is None:\n",
    "#         print(\"Error when reading \",image_path)\n",
    "#         return None\n",
    "    \n",
    "#     depth_map = None\n",
    "#     # Predict depth\n",
    "#     if depth_flag:\n",
    "#         depth_map = predict_depth(image_path,output_path)\n",
    "\n",
    "#     # Analyze person location\n",
    "#     p_surface_overlaps = analyze_person_location(obj_dict, depth_flag, image, depth_map)\n",
    "\n",
    "#     write_to_txt(image_path, output_path, p_surface_overlaps,obj_dict)\n",
    "\n",
    "#     plt.close()\n",
    "\n",
    "# analyze_road(inputfile,outputfile,obj_dict,counts ,labels, P_labels,depth_flag=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_FLAG = True\n",
    "def detect_road(image_path:str,output_path:str):\n",
    "    \"\"\"\n",
    "        This function analyzes a road scene from an image and detects various\n",
    "    entities such as roads, sidewalks, and people. \n",
    "    - detection: grounding DINO model\n",
    "    - segmentation:     SAM model \n",
    "    - depth prediction: DPT model \n",
    "    Detected entities are represented as LocationInfo objects and stored in a Counter.\n",
    "    The function also annotates the original image with detection boxes and labels.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        output_path (str): Path to save the output image with annotations and masks.\n",
    "\n",
    "    Returns:\n",
    "        obj_dict (dict): A dictionary mapping from unique entity identifiers \n",
    "        to corresponding LocationInfo objects.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the image at the given path cannot be read or processed.\n",
    "\n",
    "    Note:\n",
    "        global variables: BOX_TRESHOLD, TEXT_TRESHOLD, PED_TRESHOLD, and DEBUG,\n",
    "        need to be set prior to calling this function.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Image at path {image_path} could not be loaded. Skipping.\")\n",
    "            return None\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {image_path}. Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    ROAD_SIDEWALK = ['road', 'sidewalk'] \n",
    "    P_CLASS     = ['person','vehicle']\n",
    "    # the person label lower gDINO's performance\n",
    "    # so I split them\n",
    "\n",
    "    # detect road and sidewalk\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image,\n",
    "        classes = ROAD_SIDEWALK,\n",
    "        box_threshold= BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    detections = nms_processing(detections)\n",
    "    # detect person \n",
    "    p_detections = grounding_dino_model.predict_with_classes(\n",
    "        image = image,\n",
    "        classes = P_CLASS , \n",
    "        box_threshold= BOX_TRESHOLD,\n",
    "        text_threshold=PED_TRESHOLD \n",
    "    )\n",
    "    p_detections = nms_processing(p_detections)\n",
    "\n",
    "    # if p_detections is None:\n",
    "    #     return None\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    person_annotator = sv.BoxAnnotator()\n",
    "\n",
    "    labels = [\n",
    "        f\"{ROAD_SIDEWALK[class_id]} {i} {confidence:0.2f}\" \n",
    "        for i, (_, _, confidence, class_id, _) in enumerate(detections)]\n",
    "\n",
    "    if p_detections is None or all(confidence < 0.5 for _, _, confidence, _, _ in p_detections):\n",
    "        return None\n",
    "    else:\n",
    "        P_labels = []\n",
    "        for i, detection in enumerate(p_detections):\n",
    "            _, _, confidence, class_id, _ = detection\n",
    "            # print(\"classid is \",class_id)\n",
    "            if class_id is None:\n",
    "                continue\n",
    "            label = \"{0} {1} {2:.2f}\".format(P_CLASS[class_id], i, confidence)\n",
    "            P_labels.append(label)\n",
    "\n",
    "\n",
    "    \n",
    "    DINO_boxes = np.array(detections.xyxy)\n",
    "    P_boxes    = np.array(p_detections.xyxy)\n",
    "\n",
    "\n",
    "    annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections ,labels=labels)\n",
    "    if DEBUG:\n",
    "        sv.plot_image(annotated_frame, (16, 16))\n",
    "    person_annotation = person_annotator.annotate(scene=annotated_frame,detections= p_detections,labels= P_labels)\n",
    "    if DEBUG:\n",
    "        sv.plot_image(person_annotation, (16, 16))\n",
    "    # cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n",
    "    \n",
    "    SAM_masks = segment_ROI(sam_predictor,image,DINO_boxes)\n",
    "    P_masks = segment_ROI(sam_predictor,image,DINO_boxes)\n",
    "\n",
    "    # Create a list of LocationInfo objects for each detected object\n",
    "    obj_dict = Counter()\n",
    "    \n",
    "    for i, (box, label, mask) in enumerate(zip(DINO_boxes, labels, SAM_masks)):\n",
    "        object_type, id, confidence   = label.split(' ')\n",
    "        index = object_type +id\n",
    "        obj_dict[index] =  (LocationInfo(object_type, int(id), box, mask,confidence)) \n",
    "\n",
    "    for i, (box, label, mask) in enumerate(zip(P_boxes, P_labels, P_masks)):\n",
    "        object_type, id, confidence = label.split(' ')\n",
    "        index = object_type+id\n",
    "        obj_dict[index] = (LocationInfo(object_type, int(id), box, mask,confidence)) \n",
    "\n",
    "    if DEPTH_FLAG:\n",
    "        depth_map = predict_depth(image_path,output_path)\n",
    "    \n",
    "    # Analyze where each person is standing\n",
    "    p_surface_overlaps = []\n",
    "    \n",
    "    for name, person in obj_dict.items():\n",
    "        if person.object_type != \"person\":\n",
    "            continue # We only want to analyze persons\n",
    "        if DEPTH_FLAG:\n",
    "            person.distance = get_distance_category(depth_map,person.mask)\n",
    "            person.angle   = estimate_angle(image,person)\n",
    "        \n",
    "        overlaps = []\n",
    "        for name, surface in obj_dict.items():\n",
    "            # We only want to analyze surfaces (road or sidewalk)\n",
    "            if surface.object_type not in ROAD_SIDEWALK: \n",
    "                continue\n",
    "\n",
    "            # Check if the person and the surface overlap\n",
    "            overlap, _ = is_overlap(person.mask, surface.mask)\n",
    "            if overlap:\n",
    "                overlaps.append(surface)\n",
    "\n",
    "        p_surface_overlaps.append((person, overlaps))\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        # Print out the analysis results\n",
    "        for person, surfaces in p_surface_overlaps:\n",
    "            if surfaces:\n",
    "                surface_str = ', '.join([f\"{surface.object_type} {surface.id}\" for surface in surfaces])\n",
    "                print(f\"Person {person.id} is on the {surface_str}\")\n",
    "            else:\n",
    "                print(f\"Person {person.id} is not on any detected surface\")\n",
    "\n",
    "    # (i, j, k, d) = display_mask(SAM_masks,P_masks,P_boxes,DINO_boxes,person_annotation,output_path)\n",
    "    \n",
    "\n",
    "    write_to_txt(image_path, output_path, p_surface_overlaps, obj_dict)\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "    # return DINO_boxes,labels,P_labels,SAM_masks,P_masks\n",
    "    return obj_dict\n",
    "\n",
    "# obj_dict= detect_road(\"input/scene_2.png\",output_path=\"DINOmasked/scene_2.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"input/video_0031/image_0005.png\",output_path=\"DINOmasked/man.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"JAAD_seg_by_sec/video_0268/image_0001.png\",output_path=\"DINOmasked/video_0268/image_0001.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"JAAD_seg_by_sec/video_0268/image_0003.png\",output_path=\"DINOmasked/video_0268/image_0003.png\")\n",
    "# obj_dict,labels,p_labels =  detect_road(\"JAAD_seg_by_sec/video_0060/image_0005.png\",output_path=\"SSS.png\" )# \"DINOmasked/video_0060/image_0005.png\")\n",
    "obj_dict =  detect_road(\"input/video_2hz_output/video_0001/image_0008.png\",output_path=\"SSS.png\" )# \"DINOmasked/video_0060/image_0005.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_depth(image_path, output_path, model_path=\"DPT_module/weights/dpt_large-midas-2f21e586.pt\", model_type=\"dpt_large\", optimize=True):\n",
    "    \"\"\"\n",
    "    Predict the depth map of a single image using DPT model.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): Input image.\n",
    "        model_path (str): Path to the model weights.\n",
    "        model_type (str): Type of the DPT model to use.\n",
    "\n",
    "    Returns:\n",
    "        prediction (ndarray): The predicted depth map.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if DEBUG:\n",
    "        print(\"initialize\")\n",
    "        print(\"device: %s\" % device)\n",
    "    # load network\n",
    "    if model_type == \"dpt_large\":\n",
    "        model = DPTDepthModel(\n",
    "            path=model_path,\n",
    "            backbone=\"vitl16_384\",\n",
    "            non_negative=True,\n",
    "            enable_attention_hooks=False,\n",
    "        )\n",
    "        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Please use 'dpt_large'.\")\n",
    "\n",
    "    net_w = net_h = 384\n",
    "\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=\"minimal\",\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if optimize == True and device == torch.device(\"cuda\"):\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "        model = model.half()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # transform input\n",
    "    img = DPT_io.read_image(image_path)\n",
    "    img_input = transform({\"image\": img})[\"image\"]\n",
    "    # print(\"img.shape is\",  img.shape)\n",
    "    # compute depth map\n",
    "    with torch.no_grad():\n",
    "        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)\n",
    "\n",
    "        if optimize == True and device == torch.device(\"cuda\"):\n",
    "            sample = sample.to(memory_format=torch.channels_last)\n",
    "            sample = sample.half()\n",
    "\n",
    "        prediction = model.forward(sample)\n",
    "        prediction = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    # DPT_io.write_depth(output_path, prediction, bits=2)\n",
    "    \n",
    "    # print(\"finished\")\n",
    "    return prediction\n",
    "\n",
    "def get_distance_category(depth_map, person_mask):\n",
    "    # Define depth categories\n",
    "    categories = ['very close', 'close', 'medium', 'far', 'very far']\n",
    "\n",
    "\n",
    "    # Find the lowest true point in mask A \n",
    "    y_coords, _ = np.nonzero(person_mask)\n",
    "    lowest_point = int(np.max(y_coords) * 0.9 )#(huamn feet point)\n",
    "    # print(y_coords, lowest_point)\n",
    "    mask_a_copy = person_mask.copy().astype(bool)\n",
    "    # Slice mask A from the lowest point to the top\n",
    "    mask_a_copy[:lowest_point, :] = False\n",
    "    obj_depths = depth_map[mask_a_copy]\n",
    "      # Handle case where no depth value is available\n",
    "    if obj_depths.size == 0:\n",
    "        return \"unknown\"\n",
    "    feet_depth = np.median(obj_depths)\n",
    "\n",
    "\n",
    "    # Compute depth range of the image\n",
    "    min_depth, max_depth = np.min(depth_map), np.max(depth_map)\n",
    "\n",
    "    # Compute boundaries for each category\n",
    "    boundaries = np.linspace(min_depth, max_depth, len(categories) + 1)\n",
    "\n",
    "    # Find which category the median depth of the object belongs to\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        if boundaries[i] <= feet_depth < boundaries[i + 1]:\n",
    "            return categories[i]\n",
    "\n",
    "    return categories[-1] # If for some reason it wasn't caught in the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File IO logic\n",
    "the following code demonstrate how is the IO logic organized\n",
    "\n",
    "For the sake of fast file inquiry, I used library: Path() and os\n",
    "\n",
    "Feel free to modify this part if you need, just in case the content is too big, which may crash the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOmasked/video_0018\n",
      "DINOmasked/video_0018/Info_Video_0018.txt\n"
     ]
    }
   ],
   "source": [
    "image_path = \"input/video_0268/image_0001.png\"\n",
    "output_path = \"DINOmasked/video_0018/man.png\"\n",
    "\n",
    "output_dir = Path(output_path).parent\n",
    "\n",
    "print(output_dir)\n",
    "img_name = image_path[-8:-4]\n",
    "txt_name = \"Info_Video_\"+ str(output_dir)[-4:] +\".txt\"\n",
    "txt_path = os.path.join(output_dir, txt_name) \n",
    "print(txt_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP RUNNING THE MAIN PROGRAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start =====\n",
      "Processing:  1\n",
      "Scanning vision_output/video_0001/image_0001.png...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: input/video_2hz_output/video_0001/image_0001.png\n",
      "Processing:  2\n",
      "Scanning vision_output/video_0001/image_0002.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0002.png\n",
      "Processing:  3\n",
      "Scanning vision_output/video_0001/image_0004.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0004.png\n",
      "Processing:  4\n",
      "Scanning vision_output/video_0001/image_0005.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0005.png\n",
      "Processing:  5\n",
      "Scanning vision_output/video_0001/image_0006.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0006.png\n",
      "Processing:  6\n",
      "Scanning vision_output/video_0001/image_0007.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0007.png\n",
      "Processing:  7\n",
      "Scanning vision_output/video_0001/image_0008.png...\n",
      "Detected: input/video_2hz_output/video_0001/image_0008.png\n",
      "Processing:  8\n",
      "Scanning vision_output/video_0001/image_0009.png...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# print(f\"Image path: {os.path.basename(str(output_path))}\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScanning \u001b[39m\u001b[39m{\u001b[39;00moutput_path\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m result \u001b[39m=\u001b[39m detect_road(\u001b[39mstr\u001b[39;49m(image_path),\u001b[39mstr\u001b[39;49m(output_path))\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDetected: \u001b[39m\u001b[39m{\u001b[39;00mimage_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \n",
      "Cell \u001b[0;32mIn[69], line 113\u001b[0m, in \u001b[0;36mdetect_road\u001b[0;34m(image_path, output_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m     obj_dict[index] \u001b[39m=\u001b[39m (LocationInfo(object_type, \u001b[39mint\u001b[39m(\u001b[39mid\u001b[39m), box, mask,confidence)) \n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m DEPTH_FLAG:\n\u001b[0;32m--> 113\u001b[0m     depth_map \u001b[39m=\u001b[39m predict_depth(image_path,output_path)\n\u001b[1;32m    115\u001b[0m \u001b[39m# Analyze where each person is standing\u001b[39;00m\n\u001b[1;32m    116\u001b[0m p_surface_overlaps \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[70], line 20\u001b[0m, in \u001b[0;36mpredict_depth\u001b[0;34m(image_path, output_path, model_path, model_type, optimize)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# load network\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m model_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdpt_large\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     model \u001b[39m=\u001b[39m DPTDepthModel(\n\u001b[1;32m     21\u001b[0m         path\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[1;32m     22\u001b[0m         backbone\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvitl16_384\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m         non_negative\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     24\u001b[0m         enable_attention_hooks\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     normalization \u001b[39m=\u001b[39m NormalizeImage(mean\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Copilot/04-06-segment-anything/DPT_module/dpt/models.py:109\u001b[0m, in \u001b[0;36mDPTDepthModel.__init__\u001b[0;34m(self, path, non_negative, scale, shift, invert, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minvert \u001b[39m=\u001b[39m invert\n\u001b[1;32m     99\u001b[0m head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m    100\u001b[0m     nn\u001b[39m.\u001b[39mConv2d(features, features \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    101\u001b[0m     Interpolate(scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     nn\u001b[39m.\u001b[39mIdentity(),\n\u001b[1;32m    107\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(head, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(path)\n",
      "File \u001b[0;32m~/Copilot/04-06-segment-anything/DPT_module/dpt/models.py:49\u001b[0m, in \u001b[0;36mDPT.__init__\u001b[0;34m(self, head, features, backbone, readout, channels_last, use_bn, enable_attention_hooks)\u001b[0m\n\u001b[1;32m     42\u001b[0m hooks \u001b[39m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvitb_rn50_384\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m11\u001b[39m],\n\u001b[1;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvitb16_384\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m11\u001b[39m],\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvitl16_384\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m5\u001b[39m, \u001b[39m11\u001b[39m, \u001b[39m17\u001b[39m, \u001b[39m23\u001b[39m],\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     48\u001b[0m \u001b[39m# Instantiate backbone and reassemble blocks\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrained, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscratch \u001b[39m=\u001b[39m _make_encoder(\n\u001b[1;32m     50\u001b[0m     backbone,\n\u001b[1;32m     51\u001b[0m     features,\n\u001b[1;32m     52\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Set to true of you want to train from scratch, uses ImageNet weights\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m     groups\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m     expand\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     55\u001b[0m     exportable\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     56\u001b[0m     hooks\u001b[39m=\u001b[39;49mhooks[backbone],\n\u001b[1;32m     57\u001b[0m     use_readout\u001b[39m=\u001b[39;49mreadout,\n\u001b[1;32m     58\u001b[0m     enable_attention_hooks\u001b[39m=\u001b[39;49menable_attention_hooks,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscratch\u001b[39m.\u001b[39mrefinenet1 \u001b[39m=\u001b[39m _make_fusion_block(features, use_bn)\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscratch\u001b[39m.\u001b[39mrefinenet2 \u001b[39m=\u001b[39m _make_fusion_block(features, use_bn)\n",
      "File \u001b[0;32m~/Copilot/04-06-segment-anything/DPT_module/dpt/blocks.py:25\u001b[0m, in \u001b[0;36m_make_encoder\u001b[0;34m(backbone, features, use_pretrained, groups, expand, exportable, hooks, use_vit_only, use_readout, enable_attention_hooks)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_encoder\u001b[39m(\n\u001b[1;32m     13\u001b[0m     backbone,\n\u001b[1;32m     14\u001b[0m     features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     enable_attention_hooks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m ):\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m backbone \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvitl16_384\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m         pretrained \u001b[39m=\u001b[39m _make_pretrained_vitl16_384(\n\u001b[1;32m     26\u001b[0m             use_pretrained,\n\u001b[1;32m     27\u001b[0m             hooks\u001b[39m=\u001b[39;49mhooks,\n\u001b[1;32m     28\u001b[0m             use_readout\u001b[39m=\u001b[39;49muse_readout,\n\u001b[1;32m     29\u001b[0m             enable_attention_hooks\u001b[39m=\u001b[39;49menable_attention_hooks,\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m         scratch \u001b[39m=\u001b[39m _make_scratch(\n\u001b[1;32m     32\u001b[0m             [\u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m1024\u001b[39m], features, groups\u001b[39m=\u001b[39mgroups, expand\u001b[39m=\u001b[39mexpand\n\u001b[1;32m     33\u001b[0m         )  \u001b[39m# ViT-L/16 - 85.0% Top1 (backbone)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39melif\u001b[39;00m backbone \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvitb_rn50_384\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Copilot/04-06-segment-anything/DPT_module/dpt/vit.py:518\u001b[0m, in \u001b[0;36m_make_pretrained_vitl16_384\u001b[0;34m(pretrained, use_readout, hooks, enable_attention_hooks)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_pretrained_vitl16_384\u001b[39m(\n\u001b[1;32m    516\u001b[0m     pretrained, use_readout\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, hooks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enable_attention_hooks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    517\u001b[0m ):\n\u001b[0;32m--> 518\u001b[0m     model \u001b[39m=\u001b[39m timm\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m\"\u001b[39;49m\u001b[39mvit_large_patch16_384\u001b[39;49m\u001b[39m\"\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49mpretrained)\n\u001b[1;32m    520\u001b[0m     hooks \u001b[39m=\u001b[39m [\u001b[39m5\u001b[39m, \u001b[39m11\u001b[39m, \u001b[39m17\u001b[39m, \u001b[39m23\u001b[39m] \u001b[39mif\u001b[39;00m hooks \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m hooks\n\u001b[1;32m    521\u001b[0m     \u001b[39mreturn\u001b[39;00m _make_vit_b16_backbone(\n\u001b[1;32m    522\u001b[0m         model,\n\u001b[1;32m    523\u001b[0m         features\u001b[39m=\u001b[39m[\u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m1024\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m         enable_attention_hooks\u001b[39m=\u001b[39menable_attention_hooks,\n\u001b[1;32m    528\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_factory.py:114\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m create_fn \u001b[39m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m set_layer_config(scriptable\u001b[39m=\u001b[39mscriptable, exportable\u001b[39m=\u001b[39mexportable, no_jit\u001b[39m=\u001b[39mno_jit):\n\u001b[0;32m--> 114\u001b[0m     model \u001b[39m=\u001b[39m create_fn(\n\u001b[1;32m    115\u001b[0m         pretrained\u001b[39m=\u001b[39;49mpretrained,\n\u001b[1;32m    116\u001b[0m         pretrained_cfg\u001b[39m=\u001b[39;49mpretrained_cfg,\n\u001b[1;32m    117\u001b[0m         pretrained_cfg_overlay\u001b[39m=\u001b[39;49mpretrained_cfg_overlay,\n\u001b[1;32m    118\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    122\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/vision_transformer.py:1607\u001b[0m, in \u001b[0;36mvit_large_patch16_384\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[39mImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(patch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, embed_dim\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m-> 1607\u001b[0m model \u001b[39m=\u001b[39m _create_vision_transformer(\u001b[39m'\u001b[39;49m\u001b[39mvit_large_patch16_384\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49mpretrained, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(model_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m   1608\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/vision_transformer.py:1450\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1448\u001b[0m     _filter_fn \u001b[39m=\u001b[39m checkpoint_filter_fn\n\u001b[0;32m-> 1450\u001b[0m \u001b[39mreturn\u001b[39;00m build_model_with_cfg(\n\u001b[1;32m   1451\u001b[0m     VisionTransformer,\n\u001b[1;32m   1452\u001b[0m     variant,\n\u001b[1;32m   1453\u001b[0m     pretrained,\n\u001b[1;32m   1454\u001b[0m     pretrained_filter_fn\u001b[39m=\u001b[39;49m_filter_fn,\n\u001b[1;32m   1455\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1456\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_builder.py:381\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m# Instantiate the model\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m model_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     model \u001b[39m=\u001b[39m model_cls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    382\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     model \u001b[39m=\u001b[39m model_cls(cfg\u001b[39m=\u001b[39mmodel_cfg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/vision_transformer.py:504\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, no_embed_class, pre_norm, fc_norm, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, embed_layer, norm_layer, act_layer, block_fn, mlp_layer)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, num_classes) \u001b[39mif\u001b[39;00m num_classes \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m nn\u001b[39m.\u001b[39mIdentity()\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m weight_init \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights(weight_init)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/vision_transformer.py:512\u001b[0m, in \u001b[0;36mVisionTransformer.init_weights\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token, std\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n\u001b[0;32m--> 512\u001b[0m named_apply(get_init_weights_vit(mode, head_bias), \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[0;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m child_name, child_module \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[1;32m     33\u001b[0m     child_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin((name, child_name)) \u001b[39mif\u001b[39;00m name \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m---> 34\u001b[0m     named_apply(fn\u001b[39m=\u001b[39;49mfn, module\u001b[39m=\u001b[39;49mchild_module, name\u001b[39m=\u001b[39;49mchild_name, depth_first\u001b[39m=\u001b[39;49mdepth_first, include_root\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m depth_first \u001b[39mand\u001b[39;00m include_root:\n\u001b[1;32m     36\u001b[0m     fn(module\u001b[39m=\u001b[39mmodule, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[0;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m child_name, child_module \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[1;32m     33\u001b[0m     child_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin((name, child_name)) \u001b[39mif\u001b[39;00m name \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m---> 34\u001b[0m     named_apply(fn\u001b[39m=\u001b[39;49mfn, module\u001b[39m=\u001b[39;49mchild_module, name\u001b[39m=\u001b[39;49mchild_name, depth_first\u001b[39m=\u001b[39;49mdepth_first, include_root\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m depth_first \u001b[39mand\u001b[39;00m include_root:\n\u001b[1;32m     36\u001b[0m     fn(module\u001b[39m=\u001b[39mmodule, name\u001b[39m=\u001b[39mname)\n",
      "    \u001b[0;31m[... skipping similar frames: named_apply at line 34 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_manipulate.py:34\u001b[0m, in \u001b[0;36mnamed_apply\u001b[0;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m child_name, child_module \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mnamed_children():\n\u001b[1;32m     33\u001b[0m     child_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin((name, child_name)) \u001b[39mif\u001b[39;00m name \u001b[39melse\u001b[39;00m child_name\n\u001b[0;32m---> 34\u001b[0m     named_apply(fn\u001b[39m=\u001b[39;49mfn, module\u001b[39m=\u001b[39;49mchild_module, name\u001b[39m=\u001b[39;49mchild_name, depth_first\u001b[39m=\u001b[39;49mdepth_first, include_root\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m depth_first \u001b[39mand\u001b[39;00m include_root:\n\u001b[1;32m     36\u001b[0m     fn(module\u001b[39m=\u001b[39mmodule, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/_manipulate.py:36\u001b[0m, in \u001b[0;36mnamed_apply\u001b[0;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[1;32m     34\u001b[0m     named_apply(fn\u001b[39m=\u001b[39mfn, module\u001b[39m=\u001b[39mchild_module, name\u001b[39m=\u001b[39mchild_name, depth_first\u001b[39m=\u001b[39mdepth_first, include_root\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m depth_first \u001b[39mand\u001b[39;00m include_root:\n\u001b[0;32m---> 36\u001b[0m     fn(module\u001b[39m=\u001b[39;49mmodule, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/models/vision_transformer.py:640\u001b[0m, in \u001b[0;36minit_weights_vit_timm\u001b[0;34m(module, name)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, nn\u001b[39m.\u001b[39mLinear):\n\u001b[0;32m--> 640\u001b[0m     trunc_normal_(module\u001b[39m.\u001b[39;49mweight, std\u001b[39m=\u001b[39;49m\u001b[39m.02\u001b[39;49m)\n\u001b[1;32m    641\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m         nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mzeros_(module\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/layers/weight_init.py:67\u001b[0m, in \u001b[0;36mtrunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Fills the input Tensor with values drawn from a truncated\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mnormal distribution. The values are effectively drawn from the\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m    >>> nn.init.trunc_normal_(w)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m _trunc_normal_(tensor, mean, std, a, b)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/timm/layers/weight_init.py:28\u001b[0m, in \u001b[0;36m_trunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     24\u001b[0m u \u001b[39m=\u001b[39m norm_cdf((b \u001b[39m-\u001b[39m mean) \u001b[39m/\u001b[39m std)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Uniformly fill tensor with values from [l, u], then translate to\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# [2l-1, 2u-1].\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m l \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m u \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# standard normal\u001b[39;00m\n\u001b[1;32m     32\u001b[0m tensor\u001b[39m.\u001b[39merfinv_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dir = Path(\"input/video_2hz_output/\") # contain many folders  JAAD_seg_by_sec\n",
    "output_dir = Path('vision_output')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"===== Start =====\")\n",
    "i = 1\n",
    "# Use rglob to recursively find all image files\n",
    "for image_path in input_dir.rglob('*'):\n",
    "    if is_image_file(str(image_path)):\n",
    "        relative_path = image_path.relative_to(input_dir)\n",
    "\n",
    "        output_path = output_dir / relative_path\n",
    "        output_path.parent.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "        if False: #output_path.exists():\n",
    "            print(f\"Already scanned {output_path}, next one\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Processing: \", i)\n",
    "            i += 1\n",
    "            # print(f\"Image path: {os.path.basename(str(output_path))}\")\n",
    "            print(f\"Scanning {output_path}...\")\n",
    "            result = detect_road(str(image_path),str(output_path))\n",
    "\n",
    "            if result is not None:\n",
    "                print(f\"Detected: {image_path}\") \n",
    "            else: \n",
    "                print( \"failed to detect result\")\n",
    "\n",
    "print(\"===== END =====\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP RUNNING PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def video_diff(result_old: dict, result_new: dict):\n",
    "    \"\"\"\n",
    "    Compares two dictionaries of LocationInfo objects representing consecutive frames.\n",
    "    For each object that appears in both frames, calculates the distance moved.\n",
    "\n",
    "    Parameters:\n",
    "    - result_old: A dictionary of LocationInfo objects from the previous frame.\n",
    "    - result_new: A dictionary of LocationInfo objects from the current frame.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary mapping each object id to the distance it moved.\n",
    "    \"\"\"\n",
    "    movement_dict = {}\n",
    "\n",
    "    old_people = [obj for obj in result_old.values() if obj.object_type == 'person']\n",
    "    new_people = [obj for obj in result_new.values() if obj.object_type == 'person']\n",
    "\n",
    "    old_centroids = [[(obj.box[0] + obj.box[2]) / 2, (obj.box[1] + obj.box[3]) / 2] for obj in old_people]\n",
    "    new_centroids = [[(obj.box[0] + obj.box[2]) / 2, (obj.box[1] + obj.box[3]) / 2] for obj in new_people]\n",
    "\n",
    "    # calculate the distance matrix between all pairs of centroids\n",
    "    dist_mat = distance_matrix(old_centroids, new_centroids)\n",
    "    \n",
    "    # apply the Hungarian algorithm to find the optimal assignment\n",
    "    row_inds, col_inds = linear_sum_assignment(dist_mat)\n",
    "\n",
    "    for row_ind, col_ind in zip(row_inds, col_inds):\n",
    "        distance = dist_mat[row_ind, col_ind]\n",
    "        # here we make a new key for each matched pair\n",
    "        movement_dict[(old_people[row_ind].id, new_people[col_ind].id)] = distance\n",
    "\n",
    "    return movement_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_dir_path: str, output_dir_path: str):\n",
    "    input_dir = Path(input_dir_path) \n",
    "    output_dir = Path(output_dir_path)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"===== Start =====\")\n",
    "    image_paths = sorted(input_dir.rglob('*'), key=lambda x: x.stem)  # Use rglob to recursively find all image files and sort them\n",
    "    \n",
    "    # Check if there is at least one image in the directory\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in the directory {input_dir_path}\")\n",
    "        return\n",
    "\n",
    "    result_old = None  # Initialize the variable for the first iteration\n",
    "\n",
    "    for i, image_path in enumerate(image_paths, start=1):\n",
    "        if is_image_file(str(image_path)):\n",
    "            relative_path = image_path.relative_to(input_dir)\n",
    "\n",
    "            output_path = output_dir / relative_path\n",
    "            output_path.parent.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "            # if output_path.exists():\n",
    "            #     print(f\"Already scanned {output_path}, moving to the next one\")\n",
    "            #     continue\n",
    "\n",
    "            print(f\"Processing image {i}: {os.path.basename(str(image_path))}\")\n",
    "\n",
    "            try:\n",
    "                result_new = detect_road(str(image_path), str(output_path))\n",
    "                if result_new is not None:\n",
    "                    print(f\"Detected objects in {image_path}\")\n",
    "\n",
    "                    # Compute and compare the speed only if there is a previous frame to compare with\n",
    "                    if result_old is not None:\n",
    "                        movedict = video_diff(result_old, result_new)\n",
    "                        print(movedict)\n",
    "                    result_old = result_new\n",
    "                else: \n",
    "                    print(f\"Failed to detect objects in {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "                continue\n",
    "    print(\"===== END =====\")\n",
    "    return movedict\n",
    "\n",
    "movedict = main(\"Frame_3hz_ARHUD/video_0311/\", \"../speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movedict[(0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP file: Vision_Output_0813.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "def zip_vision_files(root_folder, output_zip_file, zip_subfolder):\n",
    "    # Create a ZIP file\n",
    "    with zipfile.ZipFile(output_zip_file, 'w') as zipf:\n",
    "        # Walk through the root folder and its subdirectories\n",
    "        for root, dirs, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.startswith(\"Info_Video_\") and file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    # Define the path inside the ZIP file\n",
    "                    zip_path = os.path.join(zip_subfolder, os.path.basename(file_path))\n",
    "                    # Add the file to the ZIP with the defined path\n",
    "                    zipf.write(file_path, zip_path)\n",
    "\n",
    "# Define the root folder where the data is stored\n",
    "root_folder = 'Vision_Output'\n",
    "\n",
    "# Get today's date in the format 0813\n",
    "date_str = datetime.now().strftime('%m%d')\n",
    "\n",
    "# Define the name of the ZIP file\n",
    "output_zip_file = f'Vision_Output_{date_str}.zip'\n",
    "\n",
    "# Define the subfolder inside the ZIP file\n",
    "zip_subfolder = f'Vision_Output_{date_str}'\n",
    "\n",
    "# Create the ZIP file\n",
    "zip_vision_files(root_folder, output_zip_file, zip_subfolder)\n",
    "\n",
    "print(f\"Created ZIP file: {output_zip_file}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
