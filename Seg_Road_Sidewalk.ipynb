{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segment Road and Sidewalk\n",
    "Tony Wang July 04 2023\n",
    "\n",
    "After semantic segmentation of road and sidewalk, we obtain the pixel level binary mask of them. Which can be used to detect human-road relationship using rule-based comparision. Since the SAM didn't provide necessary api, I write some utility func to realize it\n",
    "\n",
    "> This notebook is used for tutuorial demo, because I believe, compared to the unstable .py file, jupyter notebook would provide a vivid description and data pipeline demonstration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "# filter some annoying debug info\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import supervision as sv\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "#TODO name!\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "# import SAM_utility # \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Paths to GroundingDINO and SAM checkpoints\n",
    "GROUNDING_DINO_CONFIG_PATH = \"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = \"./weights/groundingdino_swint_ogc.pth\"\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "SAM_CHECKPOINT_PATH = \"./weights/sam_vit_b_01ec64.pth\"\n",
    "\n",
    "# Predict classes and hyper-param for GroundingDINO\n",
    "BOX_TRESHOLD = 0.25\n",
    "TEXT_TRESHOLD = 0.25\n",
    "PED_TRESHOLD = 0.5\n",
    "\n",
    "NMS_THRESHOLD = 0.85\n",
    "IOU_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "# DEBUG = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from depth_util import predict_depth,get_distance_category\n",
    "\n",
    "from mask_util import (\n",
    "    show_mask, show_points, show_box, display_mask, nms_processing, \n",
    "    is_overlap, compute_overlap, get_location, get_surface_info\n",
    ")\n",
    "from file_io    import is_image_file\n",
    "from angle_util import describe_angle,estimate_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DPT_module.dpt.models import DPTDepthModel\n",
    "from DPT_module.dpt.midas_net import MidasNet_large\n",
    "from DPT_module.dpt.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "import DPT_module.util.io as DPT_io\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model loading is quite long\n",
    "with some unremovable warning in gDINO, just ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Initialize GroundingDINO model\n",
    "grounding_dino_model = Model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG_PATH, \n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Initialize SAM model and predictor\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "LocationInfo: pack form to help data-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationInfo:\n",
    "    def __init__(self, object_type, id, box, mask,confidence):\n",
    "        self.object_type = object_type  # ('sidewalk', 'road', or 'person')\n",
    "        self.id = id  # Unique ID within the type\n",
    "        self.box = box  # Bounding box in xyxy format\n",
    "        self.mask = mask  # Binary mask indicating the precise location of the object\n",
    "        self.confidence = confidence #confidence of bbox\n",
    "        self.distance = None # str,{very close,close, median, far, very far}\n",
    "        self.angle = None    # horizontal angle relative to camera\n",
    "    def get_area(self):\n",
    "        \"\"\"\n",
    "        int: The area of the object in pixels.\n",
    "        \"\"\"\n",
    "        return np.sum(self.mask)\n",
    "# class Scene:\n",
    "#     def __init__(self):\n",
    "#         self.location_info_list = []\n",
    "\n",
    "#     def add_location_info(self, location_info):\n",
    "#         self.location_info_list.append(location_info)\n",
    "\n",
    "#     def remove_location_info(self, location_info):\n",
    "#         self.location_info_list.remove(location_info)\n",
    "\n",
    "#     def get_location_info(self, obj_type):\n",
    "#         return [info for info in self.location_info_list if info.obj_type == obj_type]\n",
    "\n",
    "#     def update_scene(self, new_location_info_list):\n",
    "#         self.location_info_list = new_location_info_list\n",
    "\n",
    "#     # Other methods can be added to manipulate and access the data as needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Architecture:\n",
    "1. gDINO : grounding_dino_model.predict_with_classes\n",
    "\n",
    "   CLASSES_prompt= ['road', 'sidewalk']\n",
    "\n",
    "   Based on testing, this pair is most reliable (otherwise the sidewalk may messed up with road) \n",
    "\n",
    "   In this part, I use the box as Region of Interest(ROI) to further prompt SAM\n",
    "\n",
    "2. Non-maximum suppression (NMS) :\n",
    "\n",
    "   get rid of redundant and overlapping bounding boxes.\n",
    "\n",
    "   the metric is Intersection over Union(IoU)\n",
    "\n",
    "3. Prompting SAM with ROI, select mask with largest area, in this step, the road and sidewalk can be segmented with naming in pixel level accuracy.\n",
    "\n",
    "4. save the result \n",
    "\n",
    "5. label the result with label and confidence\n",
    "\n",
    "6. TODO: do image sequence experiment, analyze the behavior of person\n",
    "\n",
    "7. TODO: split cases based on JAAD info\n",
    "\n",
    "   - car is moving \n",
    "   - car is stopping\n",
    "   - time\n",
    "   - weather\n",
    "   - more...\n",
    "\n",
    "In GTX3090 environment, the algorithm runs relatively fast with GPU boosting.\n",
    "\n",
    "(Not as bad as I guessed before, much faster than all of the online demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location utility function\n",
    "- is_overlap: a mask-level comparitor func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_print(obj_dict,image):\n",
    "    for name, person in obj_dict.items():\n",
    "        print(name)\n",
    "        print(person.box)\n",
    "        print(person.distance)\n",
    "        person.angle = estimate_angle(image,person)\n",
    "\n",
    "        print(f\"angle is {person.angle},it is {describe_angle(person.angle)}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_txt(image_path, output_path, p_surface_overlaps, counts, labels, p_labels,obj_dict):\n",
    "    \"\"\"\n",
    "    Writes the details of a road scene analysis into a text file. \n",
    "    The information includes:\n",
    "    detected persons and the surfaces they are on, counts of various detections,\n",
    "    labels for detections and people, and details about each object in the scene.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        image_path (str): The relative path to the input image.\n",
    "        output_path (str): The output path for the analysis results.\n",
    "        p_surface_overlaps (List[Tuple[LocationInfo, List[LocationInfo]]])\n",
    "        : A list of tuples where each tuple contains a person\n",
    "        and a list of surfaces (as LocationInfo objects) that the person overlaps with.\n",
    "        counts (Tuple[int, int, int, int]): A tuple containing counts of surface masks,\n",
    "        road & sidewalk masks, people's masks, and actual people.\n",
    "        labels (List[str]): A list of labels for the detected objects.\n",
    "        p_labels (List[str]): A list of labels specifically for the detected persons.\n",
    "        obj_dict (Dict[str, LocationInfo]) A dictionary mapping from unique entity identifiers \n",
    "        to corresponding LocationInfo objects.\n",
    "\n",
    "    Returns & Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_path).parent\n",
    "    img_name = image_path[-8:-4]\n",
    "    txt_name = \"Info_Video_\"+ str(output_dir)[-4:] +\".txt\"\n",
    "    txt_path = os.path.join(output_dir, txt_name) \n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"output_dir: \", output_dir)\n",
    "        print(\"image_name: \", img_name)\n",
    "        print(\"txt_path: \", txt_path)\n",
    "    # Check if file already exists\n",
    "    if DEBUG:\n",
    "        if os.path.exists(txt_path):\n",
    "            # Read in existing data\n",
    "            with open(txt_path, 'r') as f:\n",
    "                existing_data = f.read()\n",
    "\n",
    "            # If the info of the current image has already been recorded, return without appending\n",
    "            if f\"INFO of {img_name}:\\n\" in existing_data:\n",
    "                print(f\"ERROR: the info of{img_name} has been generated\")\n",
    "                return\n",
    "    with open(txt_path, 'a') as f: # 'a' option is for appending to the file if it exists\n",
    "        f.write(f\"INFO of {img_name}:\\n\")\n",
    "\n",
    "        get_surface_info(obj_dict,f)\n",
    "        \n",
    "        for person, surfaces in p_surface_overlaps:\n",
    "            if surfaces:\n",
    "                surface_str = ', '.join([f\"{surface.object_type} {surface.id}\" for surface in surfaces])\n",
    "                f.write(f\"Person {person.id} is on the {surface_str}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Person {person.id} is not on any detected surface\\n\")\n",
    "                \n",
    "        f.write(f\"number of Surface mask, #Road&sidewalk, People 's mask, #detected people: {counts}\\n\")\n",
    "        f.write(f\"Labels: [{', '.join(labels)}]\\n\")\n",
    "        f.write(f\"Person Labels: [{', '.join(p_labels)}]\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_ROI(sam_predictor: SamPredictor, image: np.ndarray, boxes: np.ndarray):\n",
    "    # Prompting SAM with Region of Interest\n",
    "    # Return: binary mask indicating the precise shape of object\n",
    "\n",
    "    sam_predictor.set_image(image)\n",
    "    result_masks = []\n",
    "    for box in boxes:\n",
    "        masks_np, iou_predictions, _ = sam_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=box,\n",
    "        multimask_output=True,\n",
    "        )\n",
    "        #TODO Remove the following line to get all the person masks\n",
    "        # index = np.argmax(scores_np) \n",
    "        # Add all masks to the result, not just the one with the highest score\n",
    "        # Filter out masks with IoU scores below the threshold\n",
    "        for mask, score in zip(masks_np, iou_predictions):\n",
    "            if score >= IOU_THRESHOLD:\n",
    "                result_masks.append(mask)\n",
    "\n",
    "    return np.array(result_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "DEPTH_FLAG = False\n",
    "def detect_road(image_path:str,output_path:str):\n",
    "    \"\"\"\n",
    "        This function analyzes a road scene from an image and detects various\n",
    "    entities such as roads, sidewalks, and people. \n",
    "    - detection: grounding DINO model\n",
    "    - segmentation:     SAM model \n",
    "    - depth prediction: DPT model \n",
    "    Detected entities are represented as LocationInfo objects and stored in a Counter.\n",
    "    The function also annotates the original image with detection boxes and labels.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        output_path (str): Path to save the output image with annotations and masks.\n",
    "\n",
    "    Returns:\n",
    "        obj_dict (dict): A dictionary mapping from unique entity identifiers \n",
    "        to corresponding LocationInfo objects.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the image at the given path cannot be read or processed.\n",
    "\n",
    "    Note:\n",
    "        global variables: BOX_TRESHOLD, TEXT_TRESHOLD, PED_TRESHOLD, and DEBUG,\n",
    "        need to be set prior to calling this function.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Image at path {image_path} could not be loaded. Skipping.\")\n",
    "            return None\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {image_path}. Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    ROAD_SIDEWALK = ['road', 'sidewalk'] \n",
    "    P_CLASS     = ['person'] #,'bike']\n",
    "    # the person label lower gDINO's performance\n",
    "    # so I split them\n",
    "\n",
    "    # detect road and sidewalk\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image,\n",
    "        classes = ROAD_SIDEWALK,\n",
    "        box_threshold= BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    detections = nms_processing(detections)\n",
    "    # detect person \n",
    "    p_detections = grounding_dino_model.predict_with_classes(\n",
    "        image = image,\n",
    "        classes = P_CLASS , \n",
    "        box_threshold= BOX_TRESHOLD,\n",
    "        text_threshold=PED_TRESHOLD - 0.3\n",
    "    )\n",
    "    p_detections = nms_processing(p_detections)\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    person_annotator = sv.BoxAnnotator()\n",
    "\n",
    "    labels = [\n",
    "        f\"{ROAD_SIDEWALK[class_id]} {i} {confidence:0.2f}\" \n",
    "        for i, (_, _, confidence, class_id, _) in enumerate(detections)]\n",
    "\n",
    "    P_labels = [\n",
    "        f\"{P_CLASS[class_id]} {i} {confidence:0.2f}\" \n",
    "        for i, (_, _, confidence, class_id, _) in enumerate(p_detections)]\n",
    "\n",
    "    DINO_boxes = np.array(detections.xyxy)\n",
    "    P_boxes    = np.array(p_detections.xyxy)\n",
    "    \n",
    "    annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections ,labels=labels)\n",
    "    if DEBUG:\n",
    "        sv.plot_image(annotated_frame, (16, 16))\n",
    "    person_annotation = person_annotator.annotate(scene=annotated_frame,detections= p_detections,labels= P_labels)\n",
    "    if DEBUG:\n",
    "        sv.plot_image(person_annotation, (16, 16))\n",
    "    # cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n",
    "    \n",
    "    SAM_masks = segment_ROI(sam_predictor,image,DINO_boxes)\n",
    "    P_masks = segment_ROI(sam_predictor,image,DINO_boxes)\n",
    "\n",
    "    # Create a list of LocationInfo objects for each detected object\n",
    "    obj_dict = Counter()\n",
    "    \n",
    "    for i, (box, label, mask) in enumerate(zip(DINO_boxes, labels, SAM_masks)):\n",
    "        object_type, id, confidence   = label.split(' ')\n",
    "        index = object_type +id\n",
    "        obj_dict[index] =  (LocationInfo(object_type, int(id), box, mask,confidence)) \n",
    "\n",
    "    for i, (box, label, mask) in enumerate(zip(P_boxes, P_labels, P_masks)):\n",
    "        object_type, id, confidence = label.split(' ')\n",
    "        index = object_type+id\n",
    "        obj_dict[index] = (LocationInfo(object_type, int(id), box, mask,confidence)) \n",
    "\n",
    "    if DEPTH_FLAG:\n",
    "        depth_map = predict_depth(image_path,output_path)\n",
    "    \n",
    "    # Analyze where each person is standing\n",
    "    p_surface_overlaps = []\n",
    "    \n",
    "    for name, person in obj_dict.items():\n",
    "        if person.object_type != \"person\":\n",
    "            continue # We only want to analyze persons\n",
    "        if DEPTH_FLAG:\n",
    "            person.distance = get_distance_category(depth_map,person.mask)\n",
    "            person.angle   = estimate_angle(image,person)\n",
    "        \n",
    "        overlaps = []\n",
    "        for name, surface in obj_dict.items():\n",
    "            # We only want to analyze surfaces (road or sidewalk)\n",
    "            if surface.object_type not in ROAD_SIDEWALK: \n",
    "                continue\n",
    "\n",
    "            # Check if the person and the surface overlap\n",
    "            overlap, _ = is_overlap(person.mask, surface.mask)\n",
    "            if overlap:\n",
    "                overlaps.append(surface)\n",
    "\n",
    "        p_surface_overlaps.append((person, overlaps))\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        # Print out the analysis results\n",
    "        for person, surfaces in p_surface_overlaps:\n",
    "            if surfaces:\n",
    "                surface_str = ', '.join([f\"{surface.object_type} {surface.id}\" for surface in surfaces])\n",
    "                print(f\"Person {person.id} is on the {surface_str}\")\n",
    "            else:\n",
    "                print(f\"Person {person.id} is not on any detected surface\")\n",
    "\n",
    "    (i, j, k, d) = display_mask(SAM_masks,P_masks,P_boxes,DINO_boxes,person_annotation,output_path)\n",
    "    \n",
    "\n",
    "    write_to_txt(image_path, output_path, p_surface_overlaps, (i, j, k, d), labels, P_labels,obj_dict)\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "    # return DINO_boxes,labels,P_labels,SAM_masks,P_masks\n",
    "    return obj_dict\n",
    "\n",
    "obj_dict= detect_road(\"input/scene_2.png\",output_path=\"DINO_masked/scene_2.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"input/video_0031/image_0005.png\",output_path=\"DINOmasked/man.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"JAAD_seg_by_sec/video_0268/image_0001.png\",output_path=\"DINOmasked/video_0268/image_0001.png\")\n",
    "# DINO_boxes,labels,P_labels,SAM_masks,P_masks = detect_road(\"JAAD_seg_by_sec/video_0268/image_0003.png\",output_path=\"DINOmasked/video_0268/image_0003.png\")\n",
    "# obj_dict,labels,p_labels =  detect_road(\"JAAD_seg_by_sec/video_0060/image_0005.png\",output_path=\"SSS.png\" )# \"DINOmasked/video_0060/image_0005.png\")\n",
    "# obj_dict =  detect_road(\"input/S0710/image_0005.png\",output_path=\"SSS.png\" )# \"DINOmasked/video_0060/image_0005.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File IO logic\n",
    "the following code demonstrate how is the IO logic organized\n",
    "\n",
    "For the sake of fast file inquiry, I used library: Path() and os\n",
    "\n",
    "Feel free to modify this part if you need, just in case the content is too big, which may crash the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOmasked/video_0018\n",
      "DINOmasked/video_0018/Info_Video_0018.txt\n"
     ]
    }
   ],
   "source": [
    "image_path = \"input/video_0268/image_0001.png\"\n",
    "output_path = \"DINOmasked/video_0018/man.png\"\n",
    "\n",
    "output_dir = Path(output_path).parent\n",
    "\n",
    "print(output_dir)\n",
    "img_name = image_path[-8:-4]\n",
    "txt_name = \"Info_Video_\"+ str(output_dir)[-4:] +\".txt\"\n",
    "txt_path = os.path.join(output_dir, txt_name) \n",
    "print(txt_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP RUNNING THE MAIN PROGRAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start =====\n",
      "Already scanned DINOmasked/man.png, next one\n",
      "Already scanned DINOmasked/man_black.png, next one\n",
      "Already scanned DINOmasked/scene_2.png, next one\n",
      "Already scanned DINOmasked/scene_2_black.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0003.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0001.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0002.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0006.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0004.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0005.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0008.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0007.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0009.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0011.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0012.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0010.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0014.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0013.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0015.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0017.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0016.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0018.png, next one\n",
      "Already scanned DINOmasked/S0710/image_0019.png, next one\n",
      "Already scanned DINOmasked/video_0310/image_0001.png, next one\n",
      "Already scanned DINOmasked/video_0310/image_0002.png, next one\n",
      "Already scanned DINOmasked/video_0310/image_0003.png, next one\n",
      "Already scanned DINOmasked/video_0310/image_0004.png, next one\n",
      "Already scanned DINOmasked/video_0310/image_0005.png, next one\n",
      "Already scanned DINOmasked/video_0045/image_0001.png, next one\n",
      "Already scanned DINOmasked/video_0045/image_0002.png, next one\n",
      "Already scanned DINOmasked/video_0045/image_0003.png, next one\n",
      "Already scanned DINOmasked/video_0045/image_0004.png, next one\n",
      "Already scanned DINOmasked/video_0045/image_0005.png, next one\n",
      "===== END =====\n"
     ]
    }
   ],
   "source": [
    "input_dir = Path(\"input\") # contain many folders  JAAD_seg_by_sec\n",
    "output_dir = Path('DINOmasked')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"===== Start =====\")\n",
    "i = 1\n",
    "# Use rglob to recursively find all image files\n",
    "for image_path in input_dir.rglob('*'):\n",
    "    if is_image_file(str(image_path)):\n",
    "        relative_path = image_path.relative_to(input_dir)\n",
    "\n",
    "        output_path = output_dir / relative_path\n",
    "        output_path.parent.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"Already scanned {output_path}, next one\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Processing: \", i)\n",
    "            i += 1\n",
    "            print(f\"Image path: {os.path.basename(str(image_path))}\")\n",
    "\n",
    "            result = detect_road(str(image_path),str(output_path))\n",
    "\n",
    "            if result is not None:\n",
    "                print(f\"Detected: {image_path}\") \n",
    "            else: \n",
    "                print( \"failed to detect result\")\n",
    "\n",
    "print(\"===== END =====\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir  = Path(\"input\")\n",
    "# i = 0\n",
    "# for image_path in input_dir.rglob('*'):\n",
    "#     if i > 30: break\n",
    "    \n",
    "#     if is_image_file(str(image_path)):\n",
    "#         i += 1\n",
    "\n",
    "#         print(\"image path is\" ,image_path)\n",
    "#         relative_path = image_path.relative_to(input_dir)\n",
    "\n",
    "#         output_filename = 'D_' + relative_path.name\n",
    "        \n",
    "#         output_path = Path(os.path.join( output_dir , output_filename))\n",
    "#         print(\"output_path path is\" ,output_path)\n",
    "#         output_path.parent.mkdir(parents=True,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def video_diff(result_old: dict, result_new: dict):\n",
    "    \"\"\"\n",
    "    Compares two dictionaries of LocationInfo objects representing consecutive frames.\n",
    "    For each object that appears in both frames, calculates the distance moved.\n",
    "\n",
    "    Parameters:\n",
    "    - result_old: A dictionary of LocationInfo objects from the previous frame.\n",
    "    - result_new: A dictionary of LocationInfo objects from the current frame.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary mapping each object id to the distance it moved.\n",
    "    \"\"\"\n",
    "    movement_dict = {}\n",
    "\n",
    "    old_people = [obj for obj in result_old.values() if obj.object_type == 'person']\n",
    "    new_people = [obj for obj in result_new.values() if obj.object_type == 'person']\n",
    "\n",
    "    old_centroids = [[(obj.box[0] + obj.box[2]) / 2, (obj.box[1] + obj.box[3]) / 2] for obj in old_people]\n",
    "    new_centroids = [[(obj.box[0] + obj.box[2]) / 2, (obj.box[1] + obj.box[3]) / 2] for obj in new_people]\n",
    "\n",
    "    # calculate the distance matrix between all pairs of centroids\n",
    "    dist_mat = distance_matrix(old_centroids, new_centroids)\n",
    "    \n",
    "    # apply the Hungarian algorithm to find the optimal assignment\n",
    "    row_inds, col_inds = linear_sum_assignment(dist_mat)\n",
    "\n",
    "    for row_ind, col_ind in zip(row_inds, col_inds):\n",
    "        distance = dist_mat[row_ind, col_ind]\n",
    "        # here we make a new key for each matched pair\n",
    "        movement_dict[(old_people[row_ind].id, new_people[col_ind].id)] = distance\n",
    "\n",
    "    return movement_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Start =====\n",
      "Processing image 1: image_0001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0310/image_0001.png\n",
      "Processing image 2: image_0002.png\n",
      "Detected objects in input/video_0310/image_0002.png\n",
      "{(0, 0): 92.64048392904385, (1, 2): 74.59257846161321, (2, 5): 45.75139336226269, (3, 3): 77.89570171260571, (4, 1): 730.6271619475998, (5, 4): 218.45080158112012}\n",
      "Processing image 3: image_0003.png\n",
      "Detected objects in input/video_0310/image_0003.png\n",
      "{(0, 1): 242.50690007917004, (1, 0): 26.126793277944742, (2, 3): 39.21642096404281, (3, 2): 53.13960062014486, (4, 5): 248.18324227969006, (5, 4): 240.94371144391386}\n",
      "Processing image 4: image_0004.png\n",
      "Detected objects in input/video_0310/image_0004.png\n",
      "{(0, 2): 159.6262930967252, (1, 1): 357.52577604651066, (2, 0): 34.50752157332335, (3, 3): 1.7621585070830623, (4, 4): 15.15561301328248, (5, 5): 62.14682874737308}\n",
      "Processing image 5: image_0005.png\n",
      "Detected objects in input/video_0310/image_0005.png\n",
      "{(0, 0): 29.929826726048347, (1, 6): 274.94647474617506, (2, 4): 164.67756348616723, (3, 2): 9.324654135109807, (4, 8): 15.69633609632011, (5, 5): 19.707972078199763}\n",
      "===== END =====\n"
     ]
    }
   ],
   "source": [
    "def main(input_dir_path: str, output_dir_path: str):\n",
    "    input_dir = Path(input_dir_path) \n",
    "    output_dir = Path(output_dir_path)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"===== Start =====\")\n",
    "    image_paths = sorted(input_dir.rglob('*'), key=lambda x: x.stem)  # Use rglob to recursively find all image files and sort them\n",
    "    \n",
    "    # Check if there is at least one image in the directory\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in the directory {input_dir_path}\")\n",
    "        return\n",
    "\n",
    "    result_old = None  # Initialize the variable for the first iteration\n",
    "\n",
    "    for i, image_path in enumerate(image_paths, start=1):\n",
    "        if is_image_file(str(image_path)):\n",
    "            relative_path = image_path.relative_to(input_dir)\n",
    "\n",
    "            output_path = output_dir / relative_path\n",
    "            output_path.parent.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "            if output_path.exists():\n",
    "                print(f\"Already scanned {output_path}, moving to the next one\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing image {i}: {os.path.basename(str(image_path))}\")\n",
    "\n",
    "            try:\n",
    "                result_new = detect_road(str(image_path), str(output_path))\n",
    "                if result_new is not None:\n",
    "                    print(f\"Detected objects in {image_path}\")\n",
    "\n",
    "                    # Compute and compare the speed only if there is a previous frame to compare with\n",
    "                    if result_old is not None:\n",
    "                        movedict = video_diff(result_old, result_new)\n",
    "                        print(movedict)\n",
    "                    result_old = result_new\n",
    "                else: \n",
    "                    print(f\"Failed to detect objects in {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "                continue\n",
    "    print(\"===== END =====\")\n",
    "    return\n",
    "\n",
    "main(\"input/video_0310/\", \"../speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects in input/video_0045/image_0005.png\n",
      "An error occurred while processing input/video_0045/image_0005.png: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load your video\n",
    "cap = cv2.VideoCapture('../dataset/JAAD/JAAD_clips/video_0060.mp4')\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # frames per second\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # width of the frames\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # height of the frames\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save processed video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # define the video codec\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# Check if video is opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video\")\n",
    "\n",
    "# Initialize an empty dictionary for the 'old' result\n",
    "result_old = {}\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "\n",
    "        # Convert the frame to an image file (e.g., .jpg) if required by detect_road\n",
    "        frame_path = \"temporary_frame.jpg\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "        # Call your function here\n",
    "        try:\n",
    "            result_new = detect_road(str(frame_path), \"temp_output.jpg\")\n",
    "            if result_new is not None:\n",
    "                print(f\"Detected objects in {image_path}\")\n",
    "                # Compute and compare the speed only if there is a previous frame to compare with\n",
    "                if result_old is not None:\n",
    "                    movedict = video_diff(result_old, result_new)\n",
    "                    print(movedict)\n",
    "                # Perform object tracking and update the result_old dictionary\n",
    "                result_old = result_new\n",
    "            else: \n",
    "                print(f\"Failed to detect objects in {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Read the output frame for writing to video\n",
    "        output_frame = cv2.imread(\"temp_output.jpg\")\n",
    "\n",
    "        # Write the processed frame to file\n",
    "        out.write(output_frame)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything when done\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
